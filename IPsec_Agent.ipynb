{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishown2000/AI_Project_Collab/blob/main/IPsec_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "811ac524"
      },
      "source": [
        "# Multi-Agent RAG System for IPSec using LangChain and LangGraph with FastAPI\n",
        "\n",
        "This notebook demonstrates how to build a multi-agent Retrieval-Augmented Generation (RAG) system focused on the topic of IPSec. It utilizes the LangChain and LangGraph frameworks for orchestrating different agents (one for web scraping and indexing, another for retrieval and response generation).\n",
        "\n",
        "The system is exposed as a web API using FastAPI, which is then made publicly accessible via ngrok for easy interaction, including a simple HTML/CSS/JavaScript chat interface embedded directly within the Colab output.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "*   **LangChain**: For building the RAG pipeline (Loading data, splitting text, creating embeddings, vector store).\n",
        "*   **LangGraph**: For orchestrating the workflow between different \"agents\" or steps (e.g., decide if scraping is needed, perform scraping, then retrieve and generate).\n",
        "*   **Google Generative AI**: Used for the LLM (Gemini) and Embeddings models.\n",
        "*   **ChromaDB**: An in-memory or persistent vector database for storing and searching document embeddings.\n",
        "*   **FastAPI**: A modern, fast (high-performance) web framework for building the API.\n",
        "*   **ngrok**: Used to create a secure tunnel to the local FastAPI server, making it accessible from the internet.\n",
        "*   **Playwright**: A library for robust web scraping (used by `WebBaseLoader`).\n",
        "\n",
        "The workflow allows the system to not only answer questions based on pre-indexed data but also dynamically decide if it needs to scrape more information based on the user's query and the current knowledge base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93bdbfa3"
      },
      "source": [
        "### Library Installation\n",
        "\n",
        "This cell uses the `!pip install -qU` command to quietly install or upgrade several Python libraries required for this multi-agent RAG (Retrieval-Augmented Generation) system.\n",
        "\n",
        "*   **`langchain-google-genai`**: Provides integrations with Google's Generative AI models (like Gemini) within the LangChain framework.\n",
        "*   **`beautifulsoup4`**: A library for parsing HTML and XML documents, often used by web scrapers.\n",
        "*   **`chromadb`**: The client library for Chroma, an open-source vector database used here to store and retrieve document embeddings for RAG.\n",
        "*   **`langsmith`**: A platform for observing and evaluating language model applications. The `langsmith` library enables tracing and logging of LangChain runs.\n",
        "*   **`flask`, `uvicorn`, `fastapi`, `flask-cors`**: Libraries for building and serving web applications (FastAPI is used here, with uvicorn as the ASGI server, and flask-cors for handling Cross-Origin Resource Sharing).\n",
        "*   **`python-dotenv`**: Used for loading environment variables from a `.env` file (though in Colab, `userdata` and `os.environ` are often used directly as shown later).\n",
        "*   **`langchain-community`**: Contains various third-party integrations for LangChain.\n",
        "*   **`langgraph`**: A library built on LangChain for building robust and stateful multi-actor applications by modeling steps as a graph.\n",
        "*   **`langchain-chroma`**: The specific LangChain integration for ChromaDB.\n",
        "*   **`pyngrok`**: A Python wrapper for ngrok, used to create public URLs for the locally running FastAPI server, making it accessible from outside the Colab environment.\n",
        "*   **`playwright`**: A library for browser automation, used here for potentially more robust web scraping, especially for dynamic content (though `WebBaseLoader` from `langchain_community` might use it under the hood or rely on other scrapers).\n",
        "*   **`playwright install`**: A command to install the necessary browser binaries for Playwright.\n",
        "\n",
        "The warnings about missing libraries for Playwright are common in standard Colab environments and might not prevent basic scraping, but could affect advanced browser functionalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0mdPRGwmsMm",
        "outputId": "db641b8e-52bc-4749-bbce-5799081d9544",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.7/367.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Chromium 138.0.7204.23 (playwright build v1179)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1179/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G171.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 0% 49.4s\u001b[0K\u001b[1G171.6 MiB [] 0% 22.5s\u001b[0K\u001b[1G171.6 MiB [] 0% 10.9s\u001b[0K\u001b[1G171.6 MiB [] 1% 5.9s\u001b[0K\u001b[1G171.6 MiB [] 1% 4.4s\u001b[0K\u001b[1G171.6 MiB [] 2% 3.6s\u001b[0K\u001b[1G171.6 MiB [] 3% 3.0s\u001b[0K\u001b[1G171.6 MiB [] 4% 2.7s\u001b[0K\u001b[1G171.6 MiB [] 5% 2.7s\u001b[0K\u001b[1G171.6 MiB [] 6% 2.5s\u001b[0K\u001b[1G171.6 MiB [] 7% 2.6s\u001b[0K\u001b[1G171.6 MiB [] 8% 2.4s\u001b[0K\u001b[1G171.6 MiB [] 9% 2.3s\u001b[0K\u001b[1G171.6 MiB [] 10% 2.2s\u001b[0K\u001b[1G171.6 MiB [] 11% 2.1s\u001b[0K\u001b[1G171.6 MiB [] 13% 2.0s\u001b[0K\u001b[1G171.6 MiB [] 14% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 15% 1.9s\u001b[0K\u001b[1G171.6 MiB [] 16% 1.8s\u001b[0K\u001b[1G171.6 MiB [] 17% 1.7s\u001b[0K\u001b[1G171.6 MiB [] 19% 1.7s\u001b[0K\u001b[1G171.6 MiB [] 20% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 21% 1.6s\u001b[0K\u001b[1G171.6 MiB [] 23% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 24% 1.5s\u001b[0K\u001b[1G171.6 MiB [] 25% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 26% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 27% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 28% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 29% 1.4s\u001b[0K\u001b[1G171.6 MiB [] 30% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 32% 1.3s\u001b[0K\u001b[1G171.6 MiB [] 33% 1.2s\u001b[0K\u001b[1G171.6 MiB [] 34% 1.2s\u001b[0K\u001b[1G171.6 MiB [] 35% 1.2s\u001b[0K\u001b[1G171.6 MiB [] 36% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 37% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 39% 1.1s\u001b[0K\u001b[1G171.6 MiB [] 40% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 42% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 43% 1.0s\u001b[0K\u001b[1G171.6 MiB [] 44% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 46% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 47% 0.9s\u001b[0K\u001b[1G171.6 MiB [] 49% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 50% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 52% 0.8s\u001b[0K\u001b[1G171.6 MiB [] 53% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 55% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 56% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 58% 0.7s\u001b[0K\u001b[1G171.6 MiB [] 59% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 60% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 62% 0.6s\u001b[0K\u001b[1G171.6 MiB [] 64% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 65% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 66% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 67% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 68% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 69% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 70% 0.5s\u001b[0K\u001b[1G171.6 MiB [] 71% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 72% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 73% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 74% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 75% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 76% 0.4s\u001b[0K\u001b[1G171.6 MiB [] 77% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 78% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 80% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 81% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 82% 0.3s\u001b[0K\u001b[1G171.6 MiB [] 83% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 85% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 86% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 87% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 88% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 89% 0.2s\u001b[0K\u001b[1G171.6 MiB [] 91% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 92% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 93% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 95% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 96% 0.1s\u001b[0K\u001b[1G171.6 MiB [] 97% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 98% 0.0s\u001b[0K\u001b[1G171.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 138.0.7204.23 (playwright build v1179) downloaded to /root/.cache/ms-playwright/chromium-1179\n",
            "Downloading Chromium Headless Shell 138.0.7204.23 (playwright build v1179)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1179/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G104.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 0% 30.1s\u001b[0K\u001b[1G104.5 MiB [] 0% 15.1s\u001b[0K\u001b[1G104.5 MiB [] 0% 9.3s\u001b[0K\u001b[1G104.5 MiB [] 1% 5.3s\u001b[0K\u001b[1G104.5 MiB [] 2% 3.3s\u001b[0K\u001b[1G104.5 MiB [] 3% 2.6s\u001b[0K\u001b[1G104.5 MiB [] 5% 2.1s\u001b[0K\u001b[1G104.5 MiB [] 7% 1.7s\u001b[0K\u001b[1G104.5 MiB [] 8% 1.7s\u001b[0K\u001b[1G104.5 MiB [] 9% 1.7s\u001b[0K\u001b[1G104.5 MiB [] 11% 1.6s\u001b[0K\u001b[1G104.5 MiB [] 13% 1.5s\u001b[0K\u001b[1G104.5 MiB [] 15% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 17% 1.3s\u001b[0K\u001b[1G104.5 MiB [] 19% 1.2s\u001b[0K\u001b[1G104.5 MiB [] 21% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 22% 1.1s\u001b[0K\u001b[1G104.5 MiB [] 24% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 26% 1.0s\u001b[0K\u001b[1G104.5 MiB [] 28% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 30% 0.9s\u001b[0K\u001b[1G104.5 MiB [] 33% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 34% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 36% 0.8s\u001b[0K\u001b[1G104.5 MiB [] 37% 0.7s\u001b[0K\u001b[1G104.5 MiB [] 40% 0.7s\u001b[0K\u001b[1G104.5 MiB [] 42% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 44% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 45% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 47% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 49% 0.6s\u001b[0K\u001b[1G104.5 MiB [] 52% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 54% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 57% 0.5s\u001b[0K\u001b[1G104.5 MiB [] 59% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 60% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 62% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 63% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 65% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 67% 0.4s\u001b[0K\u001b[1G104.5 MiB [] 69% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 71% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 72% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 74% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 76% 0.3s\u001b[0K\u001b[1G104.5 MiB [] 78% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 80% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 83% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 85% 0.2s\u001b[0K\u001b[1G104.5 MiB [] 87% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 90% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 91% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G104.5 MiB [] 96% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G104.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 138.0.7204.23 (playwright build v1179) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1179\n",
            "Downloading Firefox 139.0 (playwright build v1488)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1488/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G92.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G92.3 MiB [] 0% 29.2s\u001b[0K\u001b[1G92.3 MiB [] 0% 13.9s\u001b[0K\u001b[1G92.3 MiB [] 0% 8.6s\u001b[0K\u001b[1G92.3 MiB [] 1% 4.4s\u001b[0K\u001b[1G92.3 MiB [] 3% 2.7s\u001b[0K\u001b[1G92.3 MiB [] 4% 2.3s\u001b[0K\u001b[1G92.3 MiB [] 5% 1.8s\u001b[0K\u001b[1G92.3 MiB [] 7% 1.6s\u001b[0K\u001b[1G92.3 MiB [] 9% 1.5s\u001b[0K\u001b[1G92.3 MiB [] 10% 1.5s\u001b[0K\u001b[1G92.3 MiB [] 11% 1.5s\u001b[0K\u001b[1G92.3 MiB [] 12% 1.5s\u001b[0K\u001b[1G92.3 MiB [] 13% 1.4s\u001b[0K\u001b[1G92.3 MiB [] 15% 1.3s\u001b[0K\u001b[1G92.3 MiB [] 16% 1.3s\u001b[0K\u001b[1G92.3 MiB [] 18% 1.2s\u001b[0K\u001b[1G92.3 MiB [] 19% 1.2s\u001b[0K\u001b[1G92.3 MiB [] 21% 1.1s\u001b[0K\u001b[1G92.3 MiB [] 23% 1.1s\u001b[0K\u001b[1G92.3 MiB [] 24% 1.1s\u001b[0K\u001b[1G92.3 MiB [] 26% 1.0s\u001b[0K\u001b[1G92.3 MiB [] 27% 1.0s\u001b[0K\u001b[1G92.3 MiB [] 30% 0.9s\u001b[0K\u001b[1G92.3 MiB [] 32% 0.8s\u001b[0K\u001b[1G92.3 MiB [] 34% 0.8s\u001b[0K\u001b[1G92.3 MiB [] 37% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 39% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 40% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 42% 0.7s\u001b[0K\u001b[1G92.3 MiB [] 44% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 47% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 49% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 49% 0.6s\u001b[0K\u001b[1G92.3 MiB [] 51% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 53% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 56% 0.5s\u001b[0K\u001b[1G92.3 MiB [] 59% 0.4s\u001b[0K\u001b[1G92.3 MiB [] 61% 0.4s\u001b[0K\u001b[1G92.3 MiB [] 64% 0.4s\u001b[0K\u001b[1G92.3 MiB [] 66% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 68% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 71% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 73% 0.3s\u001b[0K\u001b[1G92.3 MiB [] 76% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 79% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 81% 0.2s\u001b[0K\u001b[1G92.3 MiB [] 83% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 86% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 88% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 91% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 94% 0.1s\u001b[0K\u001b[1G92.3 MiB [] 97% 0.0s\u001b[0K\u001b[1G92.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 139.0 (playwright build v1488) downloaded to /root/.cache/ms-playwright/firefox-1488\n",
            "Downloading Webkit 18.5 (playwright build v2182)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2182/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G93.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 0% 29.5s\u001b[0K\u001b[1G93.7 MiB [] 0% 14.0s\u001b[0K\u001b[1G93.7 MiB [] 0% 8.7s\u001b[0K\u001b[1G93.7 MiB [] 1% 4.4s\u001b[0K\u001b[1G93.7 MiB [] 3% 2.7s\u001b[0K\u001b[1G93.7 MiB [] 4% 2.2s\u001b[0K\u001b[1G93.7 MiB [] 5% 1.8s\u001b[0K\u001b[1G93.7 MiB [] 7% 1.6s\u001b[0K\u001b[1G93.7 MiB [] 9% 1.5s\u001b[0K\u001b[1G93.7 MiB [] 10% 1.5s\u001b[0K\u001b[1G93.7 MiB [] 11% 1.4s\u001b[0K\u001b[1G93.7 MiB [] 13% 1.3s\u001b[0K\u001b[1G93.7 MiB [] 15% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 16% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 17% 1.2s\u001b[0K\u001b[1G93.7 MiB [] 19% 1.1s\u001b[0K\u001b[1G93.7 MiB [] 21% 1.1s\u001b[0K\u001b[1G93.7 MiB [] 23% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 24% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 26% 1.0s\u001b[0K\u001b[1G93.7 MiB [] 27% 0.9s\u001b[0K\u001b[1G93.7 MiB [] 30% 0.8s\u001b[0K\u001b[1G93.7 MiB [] 32% 0.8s\u001b[0K\u001b[1G93.7 MiB [] 34% 0.8s\u001b[0K\u001b[1G93.7 MiB [] 35% 0.8s\u001b[0K\u001b[1G93.7 MiB [] 37% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 39% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 40% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 42% 0.7s\u001b[0K\u001b[1G93.7 MiB [] 44% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 46% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 48% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 50% 0.6s\u001b[0K\u001b[1G93.7 MiB [] 52% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 55% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 57% 0.5s\u001b[0K\u001b[1G93.7 MiB [] 59% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 61% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 63% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 65% 0.4s\u001b[0K\u001b[1G93.7 MiB [] 67% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 70% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 72% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 73% 0.3s\u001b[0K\u001b[1G93.7 MiB [] 76% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 77% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 79% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 82% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 83% 0.2s\u001b[0K\u001b[1G93.7 MiB [] 85% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 87% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 88% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 90% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G93.7 MiB [] 95% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 96% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G93.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.5 (playwright build v2182) downloaded to /root/.cache/ms-playwright/webkit-2182\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 2% 0.7s\u001b[0K\u001b[1G2.3 MiB [] 11% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 30% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 61% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:927:14)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1049:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1038:7)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:217:7)\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Install Libraries ---\n",
        "\n",
        "!pip install -qU langchain-google-genai beautifulsoup4 chromadb langsmith flask uvicorn python-dotenv langchain-community langgraph fastapi flask-cors langchain-chroma\n",
        "\n",
        "# For ngrok (exposing FastAPI):\n",
        "!pip install -qU pyngrok\n",
        "\n",
        "# For Playwright (robust scraping):\n",
        "!pip install -qU playwright\n",
        "!playwright install\n",
        "\n",
        "# If you prefer localtunnel over ngrok (sometimes simpler, no auth token needed)\n",
        "# !npm install -g localtunnel # Requires Node.js, so usually `ngrok` is easier in Colab if not pre-installed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b34a8cb6"
      },
      "source": [
        "### Import Libraries and Apply nest_asyncio\n",
        "\n",
        "This cell imports all the necessary classes, functions, and modules from the libraries installed in the previous step.\n",
        "\n",
        "*   It includes standard Python libraries like `os`, `getpass`, `asyncio`, and `typing` for type hinting.\n",
        "*   It imports core components from `langchain`, `langchain_google_genai`, `langchain_chroma`, and `langchain_community` for building the RAG pipeline (LLM, embeddings, text splitting, document loading, vector store, chains, prompts, messages, tools).\n",
        "*   Imports `StateGraph` and `END` from `langgraph` for defining the multi-agent workflow.\n",
        "*   Imports `ngrok` from `pyngrok` for creating the public tunnel.\n",
        "*   Imports `FastAPI`, `HTTPException`, and `BaseModel` from `fastapi` for building the web API.\n",
        "*   Imports `uvicorn` for running the FastAPI application.\n",
        "*   Imports `threading` (though `asyncio` is primarily used for concurrency here).\n",
        "*   Imports `HTML`, `display` from `IPython.display` to render HTML output directly in the Colab notebook (used for the chat UI).\n",
        "*   Imports `CORSMiddleware` from `fastapi.middleware.cors` to handle Cross-Origin Resource Sharing, allowing the frontend (UI) to communicate with the backend (FastAPI) from a different origin (domain/port).\n",
        "*   **`import nest_asyncio; nest_asyncio.apply()`**: This is crucial in environments like Google Colab where an asyncio event loop might already be running. `nest_asyncio` allows nested use of `asyncio.run`, which is necessary to run the uvicorn server (which uses asyncio) within the existing Colab environment.\n",
        "\n",
        "The `print(\"Libraries installed and imported.\")` statement confirms that the imports were successful. The `USER_AGENT` warning is informational and doesn't usually cause issues."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Install Libraries ---\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "import asyncio\n",
        "from typing import List, Dict, Any, TypedDict, Union\n",
        "from google.colab import userdata\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain_community.vectorstores import Chroma # Deprecated import\n",
        "from langchain_chroma import Chroma # Updated import\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# LangGraph imports (for orchestration)\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# For ngrok tunneling\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# FastAPI imports\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Import uvicorn\n",
        "import uvicorn\n",
        "\n",
        "# Import Thread\n",
        "import threading\n",
        "\n",
        "# For displaying HTML in Colab\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# For CORS in FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops (needed for running FastAPI inside Colab's async environment)\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"Libraries installed and imported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR9_lyn5oVPI",
        "outputId": "21c4a6c9-abb0-4e5d-949d-ae3c654daa88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af0bd336"
      },
      "source": [
        "### Environment Variable Setup\n",
        "\n",
        "This cell handles the configuration of necessary API keys and environment variables.\n",
        "\n",
        "*   It prioritizes loading sensitive keys (`GOOGLE_API_KEY`, `LANGCHAIN_API_KEY`, `LANGCHAIN_PROJECT`, `NGROK_AUTH_TOKEN`) from **Colab Secrets** using `userdata.get()`. This is the recommended secure way to store keys in Colab.\n",
        "*   If a key is not found in Secrets, it falls back to prompting the user for input using `getpass.getpass()` (for sensitive keys like API keys, which masks input) or `input()` (for less sensitive info like project name).\n",
        "*   `os.environ[...] = ...` sets these values as environment variables, which are then accessed by the libraries (like LangChain and ngrok).\n",
        "*   `os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"` enables LangSmith tracing for observing the execution of the LangChain/LangGraph components.\n",
        "*   `ngrok.set_auth_token(os.environ[\"NGROK_AUTH_TOKEN\"])` explicitly sets the ngrok authentication token using the value retrieved from environment variables. This is necessary for ngrok to establish a tunnel if you have a registered account.\n",
        "*   `os.environ[\"CHROMA_TELEMETRY_DISABLED\"] = \"True\"` disables telemetry reporting for the ChromaDB library.\n",
        "\n",
        "This cell ensures that the application has the necessary credentials and configurations to interact with external services like Google GenAI, LangSmith, and ngrok."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Environment Variable Setup for Colab ---\n",
        "# Instead of dotenv, we'll directly set or prompt\n",
        "\n",
        "# Google API Key\n",
        "# If you have it as a secret in Colab, you can access it directly.\n",
        "# Otherwise, use getpass for interactive input.\n",
        "try:\n",
        "    # Attempt to load from Colab Secrets (recommended)\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    print(\"Google API Key loaded from Colab Secrets.\")\n",
        "except Exception:\n",
        "    # Fallback to getpass if not using Colab Secrets or running locally\n",
        "    if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "        print(\"Google API Key set via input.\")\n",
        "\n",
        "# LangSmith API Key\n",
        "# Ensure you set these in your Colab secrets if you want them to persist between sessions\n",
        "# Or manually input them if you prefer.\n",
        "try:\n",
        "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "    os.environ[\"LANGCHAIN_PROJECT\"] = userdata.get('LANGCHAIN_PROJECT')\n",
        "    print(\"LangSmith environment variables loaded from Colab Secrets.\")\n",
        "except Exception:\n",
        "    print(\"LangSmith environment variables not found in Colab Secrets. Please set manually:\")\n",
        "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key: \")\n",
        "    os.environ[\"LANGCHAIN_PROJECT\"] = input(\"Enter your LangSmith Project Name (e.g., 'Colab_MultiAgentRAG'): \")\n",
        "    print(\"LangSmith environment variables set via input.\")\n",
        "\n",
        "# NGROK AUTH Token\n",
        "# If you have it as a secret in Colab, you can access it directly.\n",
        "# Otherwise, use getpass for interactive input.\n",
        "try:\n",
        "    os.environ[\"NGROK_AUTH_TOKEN\"] = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    print(\"NGROK AUTH Token loaded from Colab Secrets.\")\n",
        "except Exception:\n",
        "    if not os.environ.get(\"NGROK_AUTH_TOKEN\"):\n",
        "        os.environ[\"NGROK_AUTH_TOKEN\"] = getpass.getpass(\"Enter your NGROK AUTH Token: \")\n",
        "        print(\"NGROK AUTH Token set via input.\")\n",
        "\n",
        "# This part needs to be in its own cell usually, after the app definition\n",
        "# Set your ngrok auth token if you have one (optional for free tier, but recommended)\n",
        "# You can get it from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok.set_auth_token(os.environ[\"NGROK_AUTH_TOKEN\"]) # Use the environment variable\n",
        "\n",
        "# Disable ChromaDB telemetry\n",
        "os.environ[\"CHROMA_TELEMETRY_DISABLED\"] = \"True\"\n",
        "print(\"ChromaDB telemetry disabled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "PzzaW4s3ouV3",
        "outputId": "201c6912-e183-4b75-db90-63ec32ab4459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-2101271860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Fallback to getpass if not using Colab Secrets or running locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your Google API Key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Google API Key set via input.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             )\n\u001b[0;32m-> 1159\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04b0476"
      },
      "source": [
        "### Google Drive Mount (Optional)\n",
        "\n",
        "This cell is commented out and provides instructions on how to mount your Google Drive to the Colab environment.\n",
        "\n",
        "*   `from google.colab import drive` imports the necessary module.\n",
        "*   `drive.mount('/content/drive')` performs the mounting operation, prompting you to authorize Colab to access your Drive files.\n",
        "*   `PERSIST_DIR = \"/content/drive/MyDrive/chroma_db_multi_agent\"` defines a path on your Google Drive where you could store the ChromaDB database.\n",
        "*   The commented-out `os.makedirs(PERSIST_DIR)` would create this directory if it doesn't exist.\n",
        "\n",
        "Mounting Google Drive is useful if you want the data indexed in your ChromaDB vector store (`vectorstore`) to persist between Colab sessions. If you don't mount Drive, the database will be stored in a temporary directory (`./chroma_db_temp`) and will be lost when the Colab runtime terminates."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Google Drive Mount (Optional, for persistent ChromaDB) ---\n",
        "# If you want your RAG database to persist between Colab sessions, run this cell\n",
        "# and follow the instructions to mount your Google Drive.\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# PERSIST_DIR = \"/content/drive/MyDrive/chroma_db_multi_agent\" # Path on your Google Drive\n",
        "# if not os.path.exists(PERSIST_DIR):\n",
        "#     os.makedirs(PERSIST_DIR)\n",
        "# print(f\"ChromaDB will persist to: {PERSIST_DIR}\")"
      ],
      "metadata": {
        "id": "cYtqMLAzo3UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a440228"
      },
      "source": [
        "### Initialize LLM, Embeddings, and Vector Store\n",
        "\n",
        "This cell initializes the core components for the RAG system.\n",
        "\n",
        "*   `llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.1)`: Initializes the chat language model using Google's Gemini 1.5 Flash model. `temperature=0.1` makes the model's responses more deterministic and focused.\n",
        "*   `embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")`: Initializes the embedding model, also from Google. This model is used to convert text (document chunks and user queries) into numerical vectors (embeddings) which are then stored in the vector database.\n",
        "*   `PERSIST_DIR_CHROMA = \"./chroma_db_temp\"`: Defines the directory where the ChromaDB database will be stored. By default, it uses a temporary local directory.\n",
        "*   The commented-out section shows how you would use the `PERSIST_DIR` variable if you had mounted Google Drive and wanted to use that for persistence.\n",
        "*   `if not os.path.exists(PERSIST_DIR_CHROMA): os.makedirs(PERSIST_DIR_CHROMA)`: Creates the persistence directory if it doesn't already exist.\n",
        "*   `vectorstore = Chroma(embedding_function=embeddings, persist_directory=PERSIST_DIR_CHROMA)`: Initializes the Chroma vector store. It's configured to use the Google embeddings and the specified persistence directory.\n",
        "*   `print(...)` confirms the initialization and the location of the ChromaDB persistence directory.\n",
        "\n",
        "The errors about `chromadb.telemetry` are usually harmless and indicate an issue with sending usage data, which is disabled anyway by the environment variable set in cell 3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Initialize LLM, Embeddings, and Vector Store ---\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.1)\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# Use a temporary directory for ChromaDB if not mounting Google Drive\n",
        "# Otherwise, use the PERSIST_DIR from the Google Drive mount section\n",
        "PERSIST_DIR_CHROMA = \"./chroma_db_temp\" # Default to temporary\n",
        "# if 'PERSIST_DIR' in locals(): # Check if Google Drive path was set\n",
        "#     PERSIST_DIR_CHROMA = PERSIST_DIR\n",
        "\n",
        "if not os.path.exists(PERSIST_DIR_CHROMA):\n",
        "    os.makedirs(PERSIST_DIR_CHROMA)\n",
        "\n",
        "vectorstore = Chroma(embedding_function=embeddings, persist_directory=PERSIST_DIR_CHROMA)\n",
        "# vectorstore.persist() # Ensures directory structure is created - Deprecated\n",
        "\n",
        "print(f\"LangChain, Google models, and ChromaDB initialized. ChromaDB persistence at: {PERSIST_DIR_CHROMA}\")"
      ],
      "metadata": {
        "id": "P1ce6zBKpHSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb48c58-d31e-4cff-ee6f-a6b5162fe678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain, Google models, and ChromaDB initialized. ChromaDB persistence at: ./chroma_db_temp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45e165e7"
      },
      "source": [
        "### Agent 1: Scraping and Indexing Agent Components\n",
        "\n",
        "This cell defines the components related to the first agent, responsible for scraping web content and indexing it into the vector store.\n",
        "\n",
        "*   `initial_ipsec_urls`: A list of predefined URLs related to IPSec that will be scraped initially.\n",
        "*   `ScrapingAgentState(TypedDict)`: Defines a dictionary structure (`TypedDict`) to represent the state for a potential future, more complex LangGraph agent specifically for scraping. It includes fields for the topic, URLs to process, and status. (Note: The current implementation directly uses a function `scrape_and_index_urls_func` rather than a full LangGraph agent state and nodes, but this TypedDict hints at a possible expansion).\n",
        "*   `scrape_and_index_urls_func(urls: List[str]) -> str`: This is the core function for the scraping and indexing logic.\n",
        "    *   It iterates through the provided list of URLs.\n",
        "    *   For each URL, it uses `WebBaseLoader` from `langchain_community.document_loaders` to fetch and load the content. `loader.load()` is used for synchronous loading.\n",
        "    *   It aggregates all loaded documents (`all_documents`).\n",
        "    *   It uses `RecursiveCharacterTextSplitter` to break down the loaded documents into smaller chunks. This is important because language models have context window limits, and smaller chunks are more effective for retrieval. `chunk_size=1000` means chunks are roughly 1000 characters long, and `chunk_overlap=200` means there's a 200-character overlap between consecutive chunks to maintain context.\n",
        "    *   `vectorstore.add_documents(chunks)`: Adds the processed text chunks and their corresponding embeddings (generated by the `embeddings` model initialized in cell 5) to the ChromaDB vector store.\n",
        "    *   It returns a status message indicating success or failure.\n",
        "*   `perform_initial_scrape()`: This async function is intended to run once at the start of the application to populate the vector store with initial data from `initial_ipsec_urls`.\n",
        "    *   It checks if the vector store is empty before scraping to avoid redundant work on subsequent runs. Note the use of `vectorstore._collection.count()`, which accesses an internal ChromaDB attribute; a more robust check might involve adding metadata to documents and querying based on that.\n",
        "    *   If the vector store is empty, it calls `scrape_and_index_urls_func` with the predefined URLs.\n",
        "\n",
        "This cell sets up the foundational logic for acquiring external knowledge and making it searchable via the vector database."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Agent 1: Scraping and Indexing Agent ---\n",
        "\n",
        "initial_ipsec_urls = [\n",
        "    \"https://www.cloudflare.com/learning/network-layer/what-is-ipsec/\",\n",
        "    \"https://www.cisco.com/c/en/us/products/security/ipsec.html\",\n",
        "    \"https://en.wikipedia.org/wiki/IPsec\",\n",
        "    \"https://www.geeksforgeeks.org/ipsec-security-architecture/\",\n",
        "    \"https://www.networkacademy.io/ccna/security/ipsec-vpn\",\n",
        "    \"https://www.imperva.com/learn/security/ipsec/\"\n",
        "]\n",
        "\n",
        "class ScrapingAgentState(TypedDict):\n",
        "    \"\"\"Represents the state of the scraping agent's graph.\"\"\"\n",
        "    topic: str\n",
        "    urls_to_scrape: List[str]\n",
        "    status: str # \"pending\", \"success\", \"failed\"\n",
        "\n",
        "async def scrape_and_index_urls_func(urls: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Asynchronously scrapes content from a list of URLs, processes it, and indexes it into the RAG database.\n",
        "    Returns a status message indicating success or failure.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Scraping Agent: Initiating scrape for {len(urls)} URLs ---\")\n",
        "    try:\n",
        "        all_documents = []\n",
        "        for url in urls:\n",
        "            print(f\"Scraping: {url}\")\n",
        "            try:\n",
        "                loader = WebBaseLoader(url)\n",
        "                docs = loader.load() # Use load() for synchronous loading\n",
        "                all_documents.extend(docs)\n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping {url}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not all_documents:\n",
        "            return \"Failed to scrape any content from the provided URLs.\"\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        chunks = text_splitter.split_documents(all_documents)\n",
        "        print(f\"Split into {len(chunks)} chunks.\")\n",
        "\n",
        "        vectorstore.add_documents(chunks)\n",
        "        #vectorstore.persist()\n",
        "        print(f\"Successfully indexed {len(chunks)} chunks into the RAG database.\")\n",
        "        return f\"Successfully scraped and indexed content from {len(all_documents)} documents.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred in scraping_agent: {e}\")\n",
        "        return f\"Scraping and indexing failed: {str(e)}\"\n",
        "\n",
        "async def perform_initial_scrape():\n",
        "    \"\"\"Performs an initial scrape of predefined IPSec URLs at startup.\"\"\"\n",
        "    print(\"\\n--- Performing initial scrape of IPSec data ---\")\n",
        "    # Check if the vectorstore is already populated to avoid re-scraping on every restart\n",
        "    # This is a simple check; for large datasets, you might check a flag file or metadata.\n",
        "    try:\n",
        "        # A simple way to check if it's empty. `_collection` is an internal attribute, be cautious.\n",
        "        # A better way might be `vectorstore.get(where={\"source\": \"initial_scrape\"})` if you tagged sources.\n",
        "        # Using count() might also be async or return a future, let's keep it simple for now\n",
        "        # and assume count() is blocking or handle its potential async nature if needed.\n",
        "        # For simplicity and to match the synchronous loader.load() change, let's assume\n",
        "        # we can check the count synchronously or that the error was specifically with aload().\n",
        "        # If vectorstore._collection.count() is async, we'd need to await it.\n",
        "        # Let's try it as is first, assuming count() is synchronous or compatible.\n",
        "        if vectorstore._collection.count() == 0:\n",
        "            print(\"Vector store is empty. Proceeding with initial scrape.\")\n",
        "            # Call the now (mostly) synchronous scrape function within the async wrapper\n",
        "            status = await scrape_and_index_urls_func(initial_ipsec_urls) # Keep await here as the function is still async\n",
        "            print(f\"Initial scrape status: {status}\")\n",
        "        else:\n",
        "            print(\"Vector store already contains data. Skipping initial scrape.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking vector store or during initial scrape: {e}\")\n",
        "        print(\"Proceeding without initial scrape.\")\n",
        "\n",
        "print(\"Scraping Agent components defined.\")"
      ],
      "metadata": {
        "id": "Dui3n26jpKE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9157ee-59da-47c1-963e-41c37e6b94fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping Agent components defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "549fb7be"
      },
      "source": [
        "### Agent 2: RAG and Response Agent Components\n",
        "\n",
        "This cell defines the components for the second agent, which handles retrieving information from the vector store and generating a response to the user's query. This agent is structured as a LangGraph workflow.\n",
        "\n",
        "*   `AgentState(TypedDict)`: Defines the state for the main LangGraph workflow. This state dictionary passes information between the nodes (steps) of the graph. It includes:\n",
        "    *   `question`: The user's current query.\n",
        "    *   `chat_history`: The history of the conversation, used to provide context for the RAG retrieval.\n",
        "    *   `rag_response`: The initial response generated after the first RAG lookup.\n",
        "    *   `urls_to_scrape`: A list of URLs suggested for scraping if the RAG response is insufficient (used by the `check_for_scraping` node).\n",
        "    *   `scrape_needed`: A boolean flag indicating whether the `check_for_scraping` node determined that more data is needed.\n",
        "    *   `final_answer`: The final response to be sent to the user.\n",
        "    *   `status_message`: A string indicating the current status or result of a node's execution.\n",
        "*   `retrieve_and_generate(state: AgentState) -> AgentState`: This async function is a node in the LangGraph. It takes the current state, performs RAG, and updates the state.\n",
        "    *   It extracts the `question` and `chat_history` from the state.\n",
        "    *   `create_history_aware_retriever(...)`: Creates a LangChain retriever that first uses an LLM to analyze the chat history and current question to generate a better search query for the retriever.\n",
        "    *   `create_stuff_documents_chain(...)`: Creates a LangChain chain that takes retrieved documents and the original question/chat history and \"stuffs\" them into a prompt for the LLM to generate a final answer based on the context.\n",
        "    *   `create_retrieval_chain(...)`: Combines the history-aware retriever and the document chain into a single retrieval chain.\n",
        "    *   It `await retrieval_chain.ainvoke(...)` to asynchronously execute the RAG process.\n",
        "    *   It extracts the generated `answer` and updates the `rag_response` and `status_message` in the state. It also sets `scrape_needed` to `False` initially, as the next step is to check if scraping *is* needed.\n",
        "*   `check_for_scraping(state: AgentState) -> AgentState`: This async function is another node. It evaluates the initial RAG response to decide if more information is required.\n",
        "    *   It uses an LLM (`llm.ainvoke`) with a specific prompt to classify the `rag_response` as \"SUFFICIENT\" or \"SCRAPE_NEEDED\".\n",
        "    *   If \"SCRAPE_NEEDED\", it uses another LLM call to generate potential search queries for scraping.\n",
        "    *   It sets the `scrape_needed` flag and potentially `urls_to_scrape` in the state. Note: Currently, it hardcodes `example_ipsec_urls` regardless of the LLM's suggested queries; this could be improved to use the generated queries.\n",
        "*   `call_scraping_agent(state: AgentState) -> AgentState`: This async function node calls the `scrape_and_index_urls_func` defined in cell 6 to fetch and index new data.\n",
        "    *   It takes the `urls_to_scrape` from the state.\n",
        "    *   It `await scrape_and_index_urls_func(...)` to run the scraping process.\n",
        "    *   It updates the `status_message` based on the scraping result and sets `scrape_needed` back to `False` after the attempt.\n",
        "*   `finalize_response(state: AgentState) -> AgentState`: This async function node prepares the final answer.\n",
        "    *   In this simple implementation, it just takes the `rag_response` and sets it as the `final_answer` in the state. A more complex version might refine the answer here.\n",
        "\n",
        "These functions define the individual steps and decision points for the RAG agent's workflow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Agent 2: RAG and Response Agent ---\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"Represents the state of the overall multi-agent graph.\"\"\"\n",
        "    question: str\n",
        "    chat_history: List[Union[HumanMessage, AIMessage]]\n",
        "    rag_response: str\n",
        "    urls_to_scrape: List[str]\n",
        "    scrape_needed: bool\n",
        "    final_answer: str\n",
        "    status_message: str\n",
        "\n",
        "async def retrieve_and_generate(state: AgentState) -> AgentState:\n",
        "    \"\"\"Retrieves relevant documents from the vector store and generates a response.\"\"\"\n",
        "    print(\"\\n--- Agent 2: Retrieving and Generating Response ---\")\n",
        "    question = state[\"question\"]\n",
        "    chat_history = state[\"chat_history\"]\n",
        "\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm, vectorstore.as_retriever(), ChatPromptTemplate.from_messages([\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\"),\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    document_chain = create_stuff_documents_chain(\n",
        "        llm,\n",
        "        ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a helpful AI assistant. Answer the user's question based on the provided context. If you don't know the answer, state that you don't have enough information.\"),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"user\", \"Context:\\n{context}\"),\n",
        "        ]),\n",
        "    )\n",
        "\n",
        "    retrieval_chain = create_retrieval_chain(history_aware_retriever, document_chain)\n",
        "\n",
        "    try:\n",
        "        response = await retrieval_chain.ainvoke({\"input\": question, \"chat_history\": chat_history}) # Use ainvoke for async\n",
        "        rag_response = response[\"answer\"]\n",
        "        #print(f\"RAG Response Attempt: {rag_response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during RAG retrieval/generation: {e}\")\n",
        "        rag_response = \"I encountered an error trying to retrieve information from my database.\"\n",
        "\n",
        "    return {\"rag_response\": rag_response, \"scrape_needed\": False, \"status_message\": \"Performed initial RAG search.\"}\n",
        "\n",
        "async def check_for_scraping(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Determines if more information is needed via scraping.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Agent 2: Checking if scraping is needed ---\")\n",
        "    rag_response = state[\"rag_response\"]\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    check_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(\n",
        "            \"You are an expert evaluator. Your task is to determine if the provided \"\n",
        "            \"RAG response is sufficient to fully answer the user's question. \"\n",
        "            \"If the answer seems incomplete, too generic, or explicitly states it lacks information, \"\n",
        "            \"you should indicate that more scraping is needed. Otherwise, indicate it's sufficient.\\n\\n\"\n",
        "            \"Respond ONLY with 'SCRAPE_NEEDED' if more scraping is required, otherwise respond with 'SUFFICIENT'.\"\n",
        "        ),\n",
        "        (\"user\", f\"User Question: {question}\\n\\nExisting RAG Response: {rag_response}\\n\\nIs this response SUFFICIENT or is SCRAPE_NEEDED?\"),\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        decision = (await llm.ainvoke(check_prompt)).content.strip().upper() # Use ainvoke\n",
        "        print(f\"LLM Decision on sufficiency: {decision}\")\n",
        "        if \"SCRAPE_NEEDED\" in decision:\n",
        "            search_query_prompt = ChatPromptTemplate.from_messages([\n",
        "                SystemMessage(\n",
        "                    \"Based on the user's question and the insufficient RAG response, \"\n",
        "                    \"suggest 3-5 specific search queries (comma-separated) that would help find \"\n",
        "                    \"more comprehensive information. Focus on keywords for web scraping. \"\n",
        "                    \"Example: 'IPSec components, IPSec architecture, AH and ESP protocols'\"\n",
        "                ),\n",
        "                (\"user\", f\"User Question: {question}\\n\\nInsufficient RAG Response: {rag_response}\\n\\nSuggest search queries:\"),\n",
        "            ])\n",
        "            search_queries_str = (await llm.ainvoke(search_query_prompt)).content.strip() # Use ainvoke\n",
        "            print(f\"Suggested search queries for scraping: {search_queries_str}\")\n",
        "\n",
        "            example_ipsec_urls = [\n",
        "                \"https://www.cloudflare.com/learning/network-layer/what-is-ipsec/\",\n",
        "                \"https://www.cisco.com/c/en/us/products/security/ipsec.html\",\n",
        "                \"https://en.wikipedia.org/wiki/IPsec\",\n",
        "                \"https://www.geeksforgeeks.org/ipsec-security-architecture/\"\n",
        "            ]\n",
        "            urls_for_scraping = example_ipsec_urls\n",
        "\n",
        "            return {\"scrape_needed\": True, \"urls_to_scrape\": urls_for_scraping, \"status_message\": \"Scraping deemed necessary.\"}\n",
        "        else:\n",
        "            return {\"scrape_needed\": False, \"status_message\": \"Existing information sufficient.\"}\n",
        "    except Exception as e:\n",
        "        print(f\"Error during sufficiency check: {e}\")\n",
        "        return {\"scrape_needed\": False, \"status_message\": f\"Error during sufficiency check: {str(e)}\"}\n",
        "\n",
        "async def call_scraping_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Asynchronously calls the scraping agent to acquire and index new data.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Agent 2: Calling Scraping Agent ---\")\n",
        "    urls_to_scrape = state[\"urls_to_scrape\"]\n",
        "    if not urls_to_scrape:\n",
        "        print(\"No URLs provided for scraping. Skipping.\")\n",
        "        return {\"scrape_needed\": False, \"status_message\": \"No URLs to scrape.\"}\n",
        "\n",
        "    try:\n",
        "        scraping_result = await scrape_and_index_urls_func(urls_to_scrape)\n",
        "        print(f\"Scraping Result: {scraping_result}\")\n",
        "        if \"successfully\" in scraping_result.lower():\n",
        "            return {\"status_message\": \"Scraping completed successfully. Re-evaluating.\", \"scrape_needed\": False}\n",
        "        else:\n",
        "            return {\"status_message\": f\"Scraping failed: {scraping_result}\", \"scrape_needed\": False}\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling scraping agent function: {e}\")\n",
        "        return {\"status_message\": f\"Error calling scraping agent function: {str(e)}\", \"scrape_needed\": False}\n",
        "\n",
        "async def finalize_response(state: AgentState) -> AgentState:\n",
        "    \"\"\"Prepares the final answer for the user.\"\"\"\n",
        "    print(\"\\n--- Agent 2: Finalizing Response ---\")\n",
        "    return {\"final_answer\": state[\"rag_response\"], \"status_message\": \"Final answer prepared.\"}"
      ],
      "metadata": {
        "id": "HVXzcFh9pOD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81040702"
      },
      "source": [
        "### LangGraph Setup for Agent 2 Workflow\n",
        "\n",
        "This cell defines the structure and flow of the RAG and Response Agent using LangGraph.\n",
        "\n",
        "*   `workflow = StateGraph(AgentState)`: Initializes a `StateGraph` with the defined `AgentState`. This graph will manage the flow of the state dictionary between nodes.\n",
        "*   `workflow.add_node(\"node_name\", async_function)`: Adds each of the async functions defined in cell 7 as a named node in the graph.\n",
        "*   `workflow.set_entry_point(\"retrieve_and_generate\")`: Specifies the first node to be executed when the graph is invoked.\n",
        "*   `workflow.add_conditional_edges(...)`: Defines edges (transitions) between nodes based on the state.\n",
        "    *   From `retrieve_and_generate`, it transitions to `check_for_scraping` if `state[\"scrape_needed\"]` is true (meaning the initial RAG might be insufficient), otherwise it transitions to `finalize_response`.\n",
        "    *   From `check_for_scraping`, it transitions to `call_scraping_agent` if `state[\"scrape_needed\"]` is true (meaning scraping was deemed necessary), otherwise it transitions to `finalize_response`.\n",
        "*   `workflow.add_edge(\"call_scraping_agent\", \"retrieve_and_generate\")`: Creates a direct edge from `call_scraping_agent` back to `retrieve_and_generate`. This means after attempting to scrape and index new data, the workflow returns to the RAG step to potentially generate a better answer with the new information.\n",
        "*   `workflow.add_edge(\"finalize_response\", END)`: Defines the final step. When the state reaches the `finalize_response` node, the workflow ends (`END`).\n",
        "*   `app_langgraph = workflow.compile()`: Compiles the defined graph into a runnable object (`app_langgraph`). This optimized object can be asynchronously invoked to run the workflow.\n",
        "\n",
        "This cell effectively wires together the individual agent components into a directed graph that represents the logic flow for processing a user query, potentially performing retrieval, checking for needed information, scraping if necessary, and then finalizing a response."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. LangGraph Setup for Agent 2's Workflow ---\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"retrieve_and_generate\", retrieve_and_generate)\n",
        "workflow.add_node(\"check_for_scraping\", check_for_scraping)\n",
        "workflow.add_node(\"call_scraping_agent\", call_scraping_agent)\n",
        "workflow.add_node(\"finalize_response\", finalize_response)\n",
        "\n",
        "workflow.set_entry_point(\"retrieve_and_generate\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve_and_generate\",\n",
        "    lambda state: \"scrape_needed\" if state[\"scrape_needed\"] else \"finalize_response\",\n",
        "    {\n",
        "        \"scrape_needed\": \"check_for_scraping\",\n",
        "        \"finalize_response\": \"finalize_response\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"check_for_scraping\",\n",
        "    lambda state: \"call_scraping\" if state[\"scrape_needed\"] else \"finalize_response\",\n",
        "    {\n",
        "        \"call_scraping\": \"call_scraping_agent\",\n",
        "        \"finalize_response\": \"finalize_response\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"call_scraping_agent\", \"retrieve_and_generate\")\n",
        "workflow.add_edge(\"finalize_response\", END)\n",
        "\n",
        "app_langgraph = workflow.compile()\n",
        "\n",
        "print(\"RAG and Response Agent (LangGraph workflow) defined.\")"
      ],
      "metadata": {
        "id": "kRtdPPU4pVuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b5e4b9-b864-4c7f-979f-9e13af54e9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG and Response Agent (LangGraph workflow) defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40b462af"
      },
      "source": [
        "### Initial Scrape Execution (For Testing RAG)\n",
        "\n",
        "This commented-out cell shows how to trigger the initial data scraping process.\n",
        "\n",
        "*   `asyncio.run(perform_initial_scrape())`: Calls the `perform_initial_scrape` async function (defined in cell 6) using `asyncio.run()`. This is necessary because `perform_initial_scrape` is an async function, and `asyncio.run()` executes an async function in a synchronous context, blocking until it's complete.\n",
        "*   The purpose is to populate the vector store with initial IPSec data when the notebook is first run or when this cell is executed.\n",
        "*   The print statements indicate the start and completion of the initial scrape process.\n",
        "\n",
        "This ensures that the RAG system has some base knowledge about IPSec available from the beginning, even before any user queries are processed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Perform initial scrape at application start ---\n",
        "# This ensures data is in the vector store before the API is ready.\n",
        "# Use asyncio.run for calling async function in a sync context (like top-level script execution)\n",
        "# If this were a FastAPI `startup` event, you'd use `await perform_initial_scrape()`.\n",
        "#asyncio.run(perform_initial_scrape())\n",
        "#print(\"Initial IPSec data loaded (if not already present).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAikyELD9cdN",
        "outputId": "7b5a3ce4-dd2e-45fe-d769-f38cd8de0852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Performing initial scrape of IPSec data ---\n",
            "Vector store already contains data. Skipping initial scrape.\n",
            "Initial IPSec data loaded (if not already present).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "499dbbf3"
      },
      "source": [
        "### FastAPI Application Definition\n",
        "\n",
        "This cell defines the FastAPI web application that will expose an API endpoint for interacting with the multi-agent RAG system.\n",
        "\n",
        "*   `app = FastAPI(...)`: Initializes a FastAPI application instance with a title.\n",
        "*   **`app.add_middleware(CORSMiddleware, ...)`**: Configures Cross-Origin Resource Sharing (CORS).\n",
        "    *   `allow_origins=[\"*\"]`: Allows requests from *any* origin. This is useful in development (like in Colab with ngrok) but should be restricted in production.\n",
        "    *   `allow_credentials=True`: Allows cookies and authorization headers to be included in cross-origin requests.\n",
        "    *   `allow_methods=[\"*\"]`: Allows all standard HTTP methods (GET, POST, PUT, DELETE, OPTIONS, etc.). Explicitly allowing `OPTIONS` is often necessary for CORS preflight requests.\n",
        "    *   `allow_headers=[\"*\"]`: Allows all headers in cross-origin requests.\n",
        "    *   This middleware is crucial for the HTML/JavaScript UI (running in your browser's origin) to be able to make API calls to the FastAPI server (running locally in Colab and exposed via the ngrok tunnel).\n",
        "*   `class QueryRequest(BaseModel):`: Defines a Pydantic model for validating the incoming request body to the `/query` endpoint. It expects a JSON object with a `question` string and an optional `chat_history` list of dictionaries.\n",
        "*   `@app.post(\"/query\")`: This decorator defines an API endpoint at the `/query` path that accepts POST requests.\n",
        "*   `async def process_query(request: QueryRequest):`: This is the asynchronous function that will handle incoming POST requests to `/query`. It takes the validated request body as a `QueryRequest` object.\n",
        "    *   It processes the incoming `chat_history` from the request into LangChain's `HumanMessage` and `AIMessage` objects.\n",
        "    *   It initializes the `initial_state` dictionary for the LangGraph workflow, populating it with the user's `question` and the processed `chat_history`. `scrape_needed` is set to `True` initially to trigger the workflow's check.\n",
        "    *   `final_state = await app_langgraph.ainvoke(initial_state)`: This is the core logic, asynchronously invoking the LangGraph workflow (`app_langgraph`) with the initial state. The workflow executes its nodes and transitions based on the state.\n",
        "    *   It extracts the `final_answer` and `status_message` from the `final_state` returned by the graph.\n",
        "    *   It constructs the `updated_chat_history` to include the current question and the generated answer.\n",
        "    *   It returns a JSON response containing the original question, the generated answer, the final status message, and the updated chat history (formatted back into dictionaries).\n",
        "*   `@app.get(\"/\")`: Defines a simple GET endpoint at the root path (`/`).\n",
        "*   `async def read_root():`: Returns a simple welcome message as JSON for the root endpoint.\n",
        "\n",
        "This cell sets up the HTTP interface for the multi-agent system, allowing external clients (like the chat UI) to send queries and receive responses."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. FastAPI Application ---\n",
        "\n",
        "app = FastAPI(title=\"Multi-Agent IPsec RAG System\")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allows all origins\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],  # Allows all methods (GET, POST, PUT, DELETE, OPTIONS, etc.)\n",
        "    allow_headers=[\"*\"],  # Allows all headers\n",
        ")\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    question: str\n",
        "    chat_history: List[Dict[str, str]] = []\n",
        "\n",
        "@app.post(\"/query\")\n",
        "async def process_query(request: QueryRequest):\n",
        "    \"\"\"\n",
        "    Processes a user query using the multi-agent RAG system.\n",
        "    \"\"\"\n",
        "    initial_chat_history = []\n",
        "    for msg in request.chat_history:\n",
        "        if msg.get(\"type\") == \"Human\":\n",
        "            initial_chat_history.append(HumanMessage(content=msg.get(\"content\", \"\")))\n",
        "        elif msg.get(\"type\") == \"AI_Nishown\":\n",
        "            initial_chat_history.append(AIMessage(content=msg.get(\"content\", \"\")))\n",
        "\n",
        "    initial_state = {\n",
        "        \"question\": request.question,\n",
        "        \"chat_history\": initial_chat_history,\n",
        "        \"rag_response\": \"\",\n",
        "        \"urls_to_scrape\": [],\n",
        "        \"scrape_needed\": True, # Start with scrape_needed=True to always check initially\n",
        "        \"final_answer\": \"\",\n",
        "        \"status_message\": \"Starting query processing.\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        final_state = await app_langgraph.ainvoke(initial_state)\n",
        "\n",
        "        answer = final_state.get(\"final_answer\", \"Sorry, I could not generate a comprehensive answer.\")\n",
        "        status_message = final_state.get(\"status_message\", \"Processing complete.\")\n",
        "\n",
        "        updated_chat_history = initial_chat_history + [\n",
        "            HumanMessage(content=request.question),\n",
        "            AIMessage(content=answer)\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            \"question\": request.question,\n",
        "            \"answer\": answer,\n",
        "            \"status\": status_message,\n",
        "            \"updated_chat_history\": [{\"type\": \"Human\", \"content\": msg.content} if isinstance(msg, HumanMessage) else {\"type\": \"AI_Nishown\", \"content\": msg.content} for msg in updated_chat_history]\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during query processing: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Internal server error: {str(e)}\")\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\"message\": \"Welcome to the Multi-Agent IPsec RAG System API. Use /query to ask questions.\"}"
      ],
      "metadata": {
        "id": "9F5v211-pdfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f6f4937"
      },
      "source": [
        "### Run FastAPI with ngrok GUI (Option 1)\n",
        "\n",
        "This cell contains the code to start the FastAPI server and the ngrok tunnel, and importantly, **it also includes the HTML/CSS/JavaScript code to display the chat UI directly within the Colab output.**\n",
        "\n",
        "*   `async def run_fastapi_and_ui():`: An async function encapsulating the startup logic.\n",
        "*   `port = 8000`: Defines the port on which FastAPI will run locally.\n",
        "*   `public_url = ngrok.connect(port).public_url`: Uses `pyngrok` to start an ngrok tunnel that forwards traffic from a public ngrok URL to the local port 8000. It gets the generated public URL.\n",
        "*   `print(...)`: Prints the ngrok public URL and the URL for the FastAPI interactive documentation (Swagger UI).\n",
        "*   **`chat_ui_html = f\"\"\"...\"\"\"`**: This multiline f-string contains the full HTML, CSS (`<style>`), and JavaScript (`<script>`) for the chat interface.\n",
        "    *   The HTML defines the basic structure: a container, header, message area, and input area.\n",
        "    *   The CSS styles the elements to create a chat-like appearance, including fixing the header and input, making the message area scrollable (`overflow-y: auto`), and styling the message bubbles. Crucially, the `.chat-messages` CSS should ensure it can grow or has a defined height within the flex container to enable scrolling.\n",
        "    *   The JavaScript handles user interaction:\n",
        "        *   Getting references to UI elements.\n",
        "        *   Storing `chatHistory` to maintain conversation context.\n",
        "        *   `API_BASE_URL = '{public_url}'`: **This is key.** It dynamically inserts the ngrok public URL obtained from the Python code into the JavaScript, so the frontend knows where to send API requests.\n",
        "        *   `sendMessage()`: An async function triggered by the send button or pressing Enter. It gets the user input, adds it to the UI, disables the input/button, shows a typing indicator, makes a `fetch` POST request to the `/query` endpoint of the FastAPI server (using `API_BASE_URL`), handles the response (adding the AI's answer to the UI and updating `chatHistory`), and finally re-enables input and hides the indicator.\n",
        "        *   `addMessage()`: A helper function to append messages to the chat UI and ensure it scrolls to the bottom.\n",
        "        *   Event listeners are attached to the send button click and input field keypress (for Enter).\n",
        "*   **`display(HTML(chat_ui_html))`**: This is the command that renders the generated HTML directly in the Colab output cell. This is why you see the interactive chat interface appear below this cell's execution.\n",
        "*   `uvicorn.run(app, host=\"0.0.0.0\", port=port, log_level=\"info\")`: This starts the uvicorn ASGI server, hosting the FastAPI application (`app`) on all available interfaces (`0.0.0.0`) on the specified `port`. **This call is blocking**, meaning the cell will appear to be continuously running as long as the server is active. Stopping the cell execution will stop the server.\n",
        "*   `asyncio.run(run_fastapi_and_ui())`: Executes the main async function `run_fastapi_and_ui` in the Colab environment.\n",
        "\n",
        "This cell provides a convenient way to run the entire application (backend and frontend UI display) from a single point in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9. Run FastAPI with ngrok GUI (Option 1) ---\n",
        "# This part starts the server.\n",
        "\n",
        "async def run_fastapi_and_ui():\n",
        "    port = 8000\n",
        "    public_url = ngrok.connect(port).public_url\n",
        "    print(f\"Ngrok Tunnel URL: {public_url}\")\n",
        "    print(f\"Access FastAPI Docs at: {public_url}/docs\")\n",
        "\n",
        "    # Define the HTML/CSS/JavaScript for the chat UI\n",
        "    # Note: Using f-string for public_url in JS, correctly escaped for template literals\n",
        "    chat_ui_html = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>IPsec Chat</title>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; margin: 0; padding: 0; background-color: #f4f7f6; display: flex; justify-content: center; align-items: center; min-height: 100vh; overflow: hidden; /* Prevent body scroll */ }}\n",
        "            .chat-container {{\n",
        "                width: 100%;\n",
        "                max-width: 700px;\n",
        "                background-color: #fff;\n",
        "                border-radius: 8px;\n",
        "                box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
        "                overflow: hidden;\n",
        "                display: flex;\n",
        "                flex-direction: column;\n",
        "                height: 80vh; /* Adjust height as needed */\n",
        "                max-height: 90vh; /* Add max height for larger screens */\n",
        "            }}\n",
        "            .chat-header {{ background-color: #4CAF50; color: white; padding: 15px; text-align: center; font-size: 1.2em; flex-shrink: 0; /* Prevent header from shrinking */ }}\n",
        "            .chat-messages {{\n",
        "                flex-grow: 1;\n",
        "                padding: 20px;\n",
        "                overflow-y: auto; /* Make this area scrollable vertically */\n",
        "                background-color: #e9ecef;\n",
        "                border-bottom: 1px solid #ddd;\n",
        "                scroll-behavior: smooth; /* Smooth scrolling */\n",
        "                /* Optional: Define a min/max height here if flex-grow isn't enough */\n",
        "                min-height: 0; /* Allow flex item to shrink below content size */\n",
        "            }}\n",
        "            .message {{ margin-bottom: 15px; display: flex; }}\n",
        "            .message.user {{ justify-content: flex-end; }}\n",
        "            .message.ai {{ justify-content: flex-start; }}\n",
        "            .message-bubble {{\n",
        "                max-width: 70%;\n",
        "                padding: 10px 15px;\n",
        "                border-radius: 20px;\n",
        "                word-wrap: break-word;\n",
        "                line-height: 1.4;\n",
        "            }}\n",
        "            .message.user .message-bubble {{ background-color: #DCF8C6; color: #333; }}\n",
        "            .message.ai .message-bubble {{ background-color: #FFF; color: #333; border: 1px solid #ddd; }}\n",
        "            .chat-input-area {{ display: flex; padding: 15px; border-top: 1px solid #ddd; background-color: #f9f9f9; flex-shrink: 0; /* Prevent input area from shrinking */ }}\n",
        "            .chat-input {{ flex-grow: 1; padding: 10px; border: 1px solid #ccc; border-radius: 20px; outline: none; font-size: 1em; }}\n",
        "            .send-button {{ background-color: #4CAF50; color: white; border: none; padding: 10px 20px; border-radius: 20px; margin-left: 10px; cursor: pointer; font-size: 1em; transition: background-color 0.3s ease; }}\n",
        "            .send-button:hover {{ background-color: #45a049; }}\n",
        "            .status-message {{ text-align: center; margin-top: 10px; font-size: 0.9em; color: #555; }}\n",
        "            .typing-indicator {{ margin-left: 10px; font-size: 0.9em; color: #888; display: none; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"chat-container\">\n",
        "            <div class=\"chat-header\">IPsec Chatbot</div>\n",
        "            <div class=\"chat-messages\" id=\"chatMessages\">\n",
        "                <div class=\"message ai\"><div class=\"message-bubble\">Hello! I'm AI_Nishown a multi-agent RAG system. Ask me anything about IPSec!</div></div>\n",
        "            </div>\n",
        "            <div class=\"chat-input-area\">\n",
        "                <input type=\"text\" id=\"chatInput\" class=\"chat-input\" placeholder=\"Type your message...\">\n",
        "                <button id=\"sendButton\" class=\"send-button\">Send</button>\n",
        "                <div id=\"typingIndicator\" class=\"typing-indicator\">Thinking...</div>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <script>\n",
        "            const chatMessages = document.getElementById('chatMessages');\n",
        "            const chatInput = document.getElementById('chatInput');\n",
        "            const sendButton = document.getElementById('sendButton');\n",
        "            const typingIndicator = document.getElementById('typingIndicator');\n",
        "            let chatHistory = []; // Stores conversation for context\n",
        "\n",
        "            // Use the public_url from Python for API calls\n",
        "            const API_BASE_URL = '{public_url}';\n",
        "\n",
        "            async function sendMessage() {{\n",
        "                const message = chatInput.value.trim();\n",
        "                if (message === '') return;\n",
        "\n",
        "                // Add user message to UI\n",
        "                addMessage(message, 'user');\n",
        "                chatInput.value = ''; // Clear input\n",
        "                sendButton.disabled = true; // Disable button during processing\n",
        "                typingIndicator.style.display = 'inline'; // Show typing indicator\n",
        "\n",
        "                try {{\n",
        "                    const response = await fetch(`${{API_BASE_URL}}/query`, {{ // Note the /query endpoint\n",
        "                        method: 'POST',\n",
        "                        headers: {{\n",
        "                            'Content-Type': 'application/json'\n",
        "                        }},\n",
        "                        body: JSON.stringify({{\n",
        "                            question: message,\n",
        "                            chat_history: chatHistory\n",
        "                        }})\n",
        "                    }});\n",
        "\n",
        "                    const data = await response.json();\n",
        "\n",
        "                    if (response.ok) {{\n",
        "                        addMessage(data.answer, 'AI_Nishown');\n",
        "                        chatHistory = data.updated_chat_history; // Update chat history from backend\n",
        "                    }} else {{\n",
        "                        addMessage(`Error: ${{data.detail || response.statusText}}`, 'AI_Nishown');\n",
        "                    }}\n",
        "                }} catch (error) {{\n",
        "                    console.error('Error sending message:', error);\n",
        "                    addMessage('Sorry, an error occurred. Please try again.', 'ai');\n",
        "                }} finally {{\n",
        "                    sendButton.disabled = false; // Re-enable button\n",
        "                    typingIndicator.style.display = 'none'; // Hide typing indicator\n",
        "                    chatMessages.scrollTop = chatMessages.scrollHeight; // Scroll to bottom\n",
        "                }}\n",
        "            }}\n",
        "\n",
        "            function addMessage(text, sender) {{\n",
        "                const messageDiv = document.createElement('div');\n",
        "                messageDiv.classList.add('message', sender);\n",
        "                const bubbleDiv = document.createElement('div');\n",
        "                bubbleDiv.classList.add('message-bubble');\n",
        "                bubbleDiv.innerHTML = text; // Use innerHTML to render newlines from AI\n",
        "\n",
        "                messageDiv.appendChild(bubbleDiv);\n",
        "                chatMessages.appendChild(messageDiv);\n",
        "                chatMessages.scrollTop = chatMessages.scrollHeight; // Scroll to bottom\n",
        "            }}\n",
        "\n",
        "            sendButton.addEventListener('click', sendMessage);\n",
        "            chatInput.addEventListener('keypress', function(e) {{\n",
        "                if (e.key === 'Enter') {{\n",
        "                    sendMessage();\n",
        "                }}\n",
        "            }});\n",
        "        </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    display(HTML(chat_ui_html))\n",
        "\n",
        "    # Start Uvicorn server directly in the main Colab thread.\n",
        "    # This will block the cell, but keep the server running and accessible via ngrok.\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
        "\n",
        "# To run the FastAPI app and display the UI in Colab\n",
        "# This will start the FastAPI server and keep the Colab cell \"running\"\n",
        "# indefinitely as long as the server is active.\n",
        "asyncio.run(run_fastapi_and_ui())"
      ],
      "metadata": {
        "id": "jMPaG3OFIx23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18dec23a"
      },
      "source": [
        "### Run FastAPI with ngrok HTTPs (Option 2 - Alternative)\n",
        "\n",
        "This cell provides an alternative way to run the FastAPI application and ngrok tunnel, focusing on the server execution without embedding the UI HTML directly.\n",
        "\n",
        "*   `async def run_fastapi():`: An async function to start the server.\n",
        "*   `port = 8000`: Defines the local port.\n",
        "*   `public_url = ngrok.connect(port)`: Starts the ngrok tunnel. Note: This version of `ngrok.connect` might return an object from which you'd extract the URL (`public_url.public_url`).\n",
        "*   `print(...)`: Prints the public ngrok URL and Swagger UI link.\n",
        "*   `config = uvicorn.Config(app, host=\"0.0.0.0\", port=port, log_level=\"info\")`: Creates a Uvicorn configuration object for the FastAPI app.\n",
        "*   `server = uvicorn.Server(config)`: Creates a Uvicorn server instance.\n",
        "*   `await server.serve()`: Starts the Uvicorn server asynchronously. This line will block the cell execution until the server is stopped.\n",
        "*   `#await run_fastapi()`: The call to execute the async function is commented out.\n",
        "\n",
        "This cell is an alternative to the previous one if you wanted to run just the backend server and access it, for example, from an external application or perhaps a separate HTML file loaded elsewhere, rather than having the UI displayed directly in the Colab output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9. Run FastAPI with ngrok HTTPs (Option 2) ---\n",
        "\n",
        "print(\"FastAPI app defined. Now starting Uvicorn and ngrok...\")\n",
        "\n",
        "async def run_fastapi():\n",
        "    port = 8000\n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(f\"Ngrok Tunnel URL: {public_url}\")\n",
        "    print(f\"Access Swagger UI at: {public_url}/docs\")\n",
        "\n",
        "    # Start Uvicorn server in a separate thread/process\n",
        "    # Use `use_subprocess=False` for Colab to avoid issues with separate processes\n",
        "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
        "    server = uvicorn.Server(config)\n",
        "    await server.serve()\n",
        "\n",
        "# To run the FastAPI app in Colab\n",
        "# Await the coroutine directly in the notebook environment\n",
        "#await run_fastapi()"
      ],
      "metadata": {
        "id": "xXfI9_h3pj1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4048d82b-4254-43e2-ddfd-c07a8599c906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI app defined. Now starting Uvicorn and ngrok...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples for Alternative 2"
      ],
      "metadata": {
        "id": "RISksf1cjdA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **First Query:**\n",
        "```\n",
        "curl -X POST \"http://127.0.0.1:8000/query\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d '{\n",
        "  \"question\": \"What are the main components of IPSec?\"\n",
        "}'\n",
        "```\n",
        "\n",
        "# **Second Query:**\n",
        "```\n",
        "curl -X POST \"http://127.0.0.1:8000/query\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d '{\n",
        "  \"question\": \"Tell me about the Authentication Header (AH) and Encapsulating Security Payload (ESP) in IPSec.\",\n",
        "  \"chat_history\": [\n",
        "    {\"type\": \"human\", \"content\": \"What are the main components of IPSec?\"},\n",
        "    {\"type\": \"ai\", \"content\": \"IPSec has several key components, including security protocols like Authentication Header (AH) and Encapsulating Security Payload (ESP), Security Associations (SAs), and key management protocols like IKE. Would you like more detail on any of these?\"}\n",
        "  ]\n",
        "}'\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "F3xZHbp93IlJ"
      }
    }
  ]
}